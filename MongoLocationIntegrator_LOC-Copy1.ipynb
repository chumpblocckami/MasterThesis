{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import json\n",
    "import bz2\n",
    "import pymongo \n",
    "import dns\n",
    "\n",
    "mongo_psswd = open('files/mongo_psswd.txt','r').readline().strip()\n",
    "cluster = pymongo.MongoClient('mongodb+srv://ciotolaaaa:'+mongo_psswd+'@mmazzola29-vgdjv.mongodb.net/TEST_COVID?retryWrites=true&w=majority')\n",
    "db = cluster.TEST_COVID\n",
    "collections_ = db.TEST_COVID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrive metadati (CONNECTION LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#query all e poi vediamo\n",
    "query = collections_.find()\n",
    "\n",
    "data = [x for x in query]\n",
    "\n",
    "import requests\n",
    "import tweepy\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime \n",
    "\n",
    "consumer_key = \"----\"\n",
    "consumer_secret = \"----\"\n",
    "access_token = \"----\"\n",
    "access_token_secret = \"----\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "print(datetime.now())\n",
    "past = datetime.now()\n",
    "log = []\n",
    "for n,tw in enumerate(data):\n",
    "    try:\n",
    "        tweet = api.get_status(tw[\"id\"])\n",
    "        base = tweet._json\n",
    "        collections_.update_one({\"id\": tw[\"id\"] },{\"$set\": {\"location\": base[\"user\"][\"location\"]}})\n",
    "        collections_.update_one({\"id\": tw[\"id\"] },{\"$set\": {\"timezone\": base[\"user\"][\"time_zone\"]}})\n",
    "        collections_.update_one({\"id\": tw[\"id\"] },{\"$set\": {\"geo\": base[\"geo\"]}})\n",
    "        collections_.update_one({\"id\": tw[\"id\"] },{\"$set\": {\"coordinates\": base[\"coordinates\"]}})\n",
    "        collections_.update_one({\"id\": tw[\"id\"] },{\"$set\": {\"place\":base[\"place\"]}}) \n",
    "    except:\n",
    "        if \"id\" in tw.keys():\n",
    "            print(tw[\"id\"],\"NOT FOUND\",end=\"\\r\")\n",
    "        else:\n",
    "            print(tw)\n",
    "        log.append(tw)\n",
    "        pass\n",
    "    if(n%100==0):\n",
    "        print(\"CHECKPOINT\",n,tw[\"id\"])\n",
    "    time.sleep(0.001)\n",
    "        \n",
    "print(datetime.now())\n",
    "print(\"0.1k tweet in:\",round((datetime.now()-past).seconds/60,2),\"minuti\")\n",
    "print(\"N skipped:\",len(log),len(log)/len(data)*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creazione Grafo dei retweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "days = collections_.find( { \"created_at\": { \"$regex\": \"Wed Aug 12*\" } } )\n",
    "days = [x for x in days]\n",
    "print(\"N. output query\",len(days))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#creazione grafo retweeted\n",
    "users_retweeted = {}\n",
    "for _ in days:\n",
    "    user = _[\"user\"]\n",
    "    \n",
    "    if user in users_retweeted.keys():\n",
    "        values = users_retweeted[user]\n",
    "        values.append(_[\"retweeted\"])\n",
    "        users_retweeted[user] = values\n",
    "    else:\n",
    "        if _[\"retweeted\"]!= None:\n",
    "            users_retweeted[user] = [_[\"retweeted\"]]\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "G = nx.DiGraph(users_retweeted)\n",
    "pos=nx.spring_layout(G)\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "plt.title(\"Wed Aug 12\")\n",
    "\n",
    "nx.draw_networkx_nodes(G,pos,node_size=10)\n",
    "print(\"DONE:nodes\")\n",
    "\n",
    "nx.draw_networkx_edges(G,pos,width=0.5);\n",
    "print(\"DONE:edges\")\n",
    "\n",
    "#highlight only hub\n",
    "labs = {n:lab for n,lab in users_retweeted.items() if n in pos and len(users_retweeted[n])>=2}\n",
    "nx.draw_networkx_nodes(labs,pos,node_size=10, node_color = \"red\")\n",
    "#nx.draw_networkx_labels(G,pos,font_size=7,labels=labs);\n",
    "print(\"DONE:Highlights\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizzazione lingua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from cleantext import clean\n",
    "def default_clean(text,lang):\n",
    "    return clean(text,\n",
    "                 fix_unicode=True,  # fix various unicode errors\n",
    "                 to_ascii=True,  # transliterate to closest ASCII representation\n",
    "                 lower=True,  # lowercase text\n",
    "                 no_line_breaks=False,  # fully strip line breaks as opposed to only normalizing them\n",
    "                 no_urls=True,  # replace all URLs with a special token\n",
    "                 no_emails=True,  # replace all email addresses with a special token\n",
    "                 no_phone_numbers=True,  # replace all phone numbers with a special token\n",
    "                 no_numbers=False,  # replace all numbers with a special token\n",
    "                 no_digits=False,  # replace all digits with a special token\n",
    "                 no_currency_symbols=True,  # replace all currency symbols with a special token\n",
    "                 no_punct=False,  # fully remove punctuation\n",
    "                 replace_with_url=\"<URL>\",\n",
    "                 replace_with_email=\"<EMAIL>\",\n",
    "                 replace_with_phone_number=\"<PHONE>\",\n",
    "                 replace_with_number=\"<NUMBER>\",\n",
    "                 replace_with_digit=\"0\",\n",
    "                 replace_with_currency_symbol=\"<CUR>\",\n",
    "                 lang=lang)\n",
    "texts=[]\n",
    "for i in days:\n",
    "    try:\n",
    "        texts.append((default_clean(i[\"text\"],i[\"lang\"]),i[\"lang\"]))\n",
    "    except:\n",
    "        pass\n",
    "print(\"Lunghezza testi analizzabili\", len(texts),round(len(texts)/len(days)*100),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import requests\n",
    "import json\n",
    "from googletrans import Translator, constants\n",
    "\n",
    "class MySentimentAnalyzer():\n",
    "    traslator = Traslator()\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    def getPolarity(self,input_text,language,\n",
    "                    label_encoded=False, \n",
    "                    use_google = True,\n",
    "                    verbose=False,\n",
    "                    *args, **kwargs):\n",
    "\n",
    "        '''\n",
    "          Returns the average polarity of a sentence. \\n\n",
    "          input_text: input string for performing sentiment analysis \\n\n",
    "          label_encoded: outputs class of text [positive,negative,neutral] \\n\n",
    "          analyzer: choose analyzer. Used VADER\n",
    "          verbose: verbose\n",
    "        '''\n",
    "\n",
    "        if language != 'en':\n",
    "            if use_google == True:\n",
    "                translation = self.translator.translate(input_text,src=lang,dest=\"en\")\n",
    "                text = translation.text\n",
    "            else:\n",
    "                from_lang  = \"en\"\n",
    "                api_url = \"http://mymemory.translated.net/api/get?q={}&langpair={}|{}\".format(input_text, language, from_lang)\n",
    "                hdrs = {      'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "                            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "                            'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "                            'Accept-Encoding': 'none',\n",
    "                            'Accept-Language': 'en-US,en;q=0.8',\n",
    "                            'Connection': 'keep-alive'}\n",
    "                response = requests.get(api_url, headers=hdrs)\n",
    "                response_json = json.loads(response.text)\n",
    "                text = response_json[\"responseData\"][\"translatedText\"]\n",
    "\n",
    "        else:\n",
    "            text = input_text\n",
    "\n",
    "            sents = sent_tokenize(text)\n",
    "            polarity = 0\n",
    "            for sent in sents:\n",
    "                score =  self.analyzer.polarity_scores(sent)[\"compound\"]\n",
    "                polarity += score\n",
    "                if verbose: \n",
    "                    print((sent,score))\n",
    "                normalized_polarity = polarity/len(sents)\n",
    "                if label_encoded:\n",
    "                    if normalized_polarity >= 0.05:\n",
    "                        return 'positive'\n",
    "                    elif normalized_polarity <= -0.05:\n",
    "                        return 'negative'\n",
    "                    else:\n",
    "                        return 'neutral'\n",
    "                else:\n",
    "                    return normalized_polarity\n",
    "\n",
    "nlp = MySentimentAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "days_en = [x for x in days if x[\"lang\"] == \"en\"]\n",
    "texts_en = [default_clean(i[\"text\"],i[\"lang\"]) for i in days_en]\n",
    "\n",
    "date = [datetime.datetime.strptime(x[\"created_at\"],'%a %b %d %H:%M:%S +0000 %Y') for x in days_en]\n",
    "sentiment = [nlp.getPolarity(input_text=x[0],language=x[1]) for x in texts[:100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([date,[x[0] for x in texts_en],[x[1] for x in texts_en],sentiment]).T\n",
    "df.columns=[\"date\",\"text\",\"lang\",\"sentiment\"]\n",
    "df[\"hour\"] = [x.minute for x in df.date.ravel()]\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "plt.title(\"Sentiment analysis by hour\")\n",
    "plt.scatter(df[\"hour\"].unique(),df.groupby(\"hour\")[\"sentiment\"].sum())\n",
    "plt.ylim([-1,1])\n",
    "plt.xlim([0,23]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUM UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get graph\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pymongo \n",
    "\n",
    "def createGraph(dtime,show=False):\n",
    "    \n",
    "    #...conversione....\n",
    "    dtconverted = datetime.strftime(datetime.strptime(dtime,'%Y-%m-%d %H:%M:%S'), '%a %b %d %H:%M:%S +0000 %Y')[:10]\n",
    "    \n",
    "    #query the dataset\n",
    "    days = collections_.find( { \"created_at\": { \"$regex\": dtconverted+\"*\" } } )\n",
    "    days = [x for x in days]\n",
    "    print(\"N. output query\",len(days))\n",
    "    \n",
    "    #creazione grafo retweeted\n",
    "    users_retweeted = {}\n",
    "    for _ in days:\n",
    "        user = _[\"user\"]\n",
    "\n",
    "        if user in users_retweeted.keys():\n",
    "            values = users_retweeted[user]\n",
    "            values.append(_[\"retweeted\"])\n",
    "            users_retweeted[user] = values\n",
    "        else:\n",
    "            if _[\"retweeted\"]!= None:\n",
    "                users_retweeted[user] = [_[\"retweeted\"]]\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "    #output\n",
    "    G = nx.DiGraph(users_retweeted)\n",
    "    if show:\n",
    "        pos=nx.spring_layout(G)\n",
    "\n",
    "        plt.figure(figsize=(9,9))\n",
    "        plt.title(str(dconverted))\n",
    "        nx.draw_networkx_nodes(G,pos,node_size=10)\n",
    "\n",
    "        nx.draw_networkx_edges(G,pos,width=0.5);\n",
    "\n",
    "        #highlight hub\n",
    "        labs = {n:lab for n,lab in users_retweeted.items() if n in pos and len(users_retweeted[n])>=2}\n",
    "        nx.draw_networkx_nodes(labs,pos,node_size=10, node_color = \"red\")\n",
    "\n",
    "        plt.show()\n",
    "    nx.write_gexf(G,\"./Prova.gexf\")\n",
    "    return users_retweeted\n",
    "\n",
    "#from last day to now \n",
    "dtimes = []\n",
    "for i in range(40):\n",
    "    now = datetime.now()\n",
    "    day_start = now.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    day_end = day_start + timedelta(hours=-(24*i))\n",
    "    dtimes.append(day_end)\n",
    "\n",
    "for datetime in dtimes:\n",
    "    createGraph(datetime[::-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
